Author: Zhenduo Wang

#############################################################################################
Subtask A Results
#############################################################################################
Logistic Regression Classifier:
Mean Accuracy:
0.650227002138
p=:0.6149870801033591
r=:0.6432432432432432
f=:0.6287978863936591
Confusion Matrix:
predict 0	1
true 0	[248, 	149]
     1	[132, 	238]
Feature weights:
	-0.3289	celebrity      
	-0.1794	swear words    
	-0.1478	URLs           
	-0.1171	word count     
	-0.1116	similarity     
	-0.0077	laughter       
	0.0601	stopwords      
	0.1829	named entity   
	0.1991	subjectivity   
	0.3178	punctuation    
	0.3447	interjections  
	0.3924	politcal       
	0.5366	polarity       
	0.8103	preposition    
	0.8147	adjective/adverb
	0.9022	discourse marker
	1.0468	intensifier    
#############################################################################################
Support Vector Machine Classifier:
Mean Accuracy:
0.595981500657
p=:0.5457809694793537
r=:0.8216216216216217
f=:0.6558791801510249
Confusion Matrix:
predict 0	1
true 0	[144, 	253]
     1	[66, 	304]
#############################################################################################
Random Forest Classifier:
Mean Accuracy:
0.621805223358
p=:0.5828025477707006
r=:0.4945945945945946
f=:0.5350877192982456
Confusion Matrix:
predict 0	1
true 0	[266, 	131]
     1	[187, 	183]
#############################################################################################
Voted Classifier:
Mean Accuracy:
0.644494776189
p=:0.6018518518518519
r=:0.7027027027027027
f=:0.6483790523690772
Confusion Matrix:
predict 0	1
true 0 	[225, 	172]
     1	[110, 	260]

#############################################################################################
Subtask B Results
#############################################################################################
Logistic Regression Classifier:
Mean Accuracy:
0.602757767298
p=:0.38831097142449145
r=:0.35438430154008405
f=:0.3451716567922133
Confusion Matrix:
predict 0	1	2	3
true 0	[297, 	98, 	2, 	0]
     1	[103, 	160, 	4, 	0]
     2	[37, 	16, 	4, 	0]
     3	[30, 	15, 	1, 	0]
Feature weights:
	-1.0468	intensifier    
	-0.9022	discourse marker
	-0.8147	adjective/adverb
	-0.8104	preposition    
	-0.5366	polarity       
	-0.3923	politcal       
	-0.3447	interjections  
	-0.3178	punctuation    
	-0.1992	subjectivity   
	-0.1829	named entity   
	-0.0601	stopwords      
	0.0076	laughter       
	0.1116	similarity     
	0.1171	word count     
	0.1478	URLs           
	0.1794	swear words    
	0.3288	celebrity      
Support Vector Machine Classifier:
Mean Accuracy:
0.544601590451
p=:0.3039860538710627
r=:0.28577156388267816
f=:0.24940065528355665
Confusion Matrix:
predict 0	1	2	3
true 0	[375, 	22, 	0, 	0]
     1	[214, 	53, 	0, 	0]
     2	[55, 	2, 	0, 	0]
     3	[44, 	2, 	0, 	0]
Random Forest Classifier:
Mean Accuracy:
0.583712682603
p=:0.3031500967933606
r=:0.31039356875760005
f=:0.2942614614301361
Confusion Matrix:
predict 0	1	2	3
true 0	[309, 	83, 	5, 	0]
     1	[145, 	119, 	3, 	0]
     2	[41, 	15, 	1, 	0]
     3	[32, 	14, 	0, 	0]
Voted Classifier:
Mean Accuracy:
0.606921361384
p=:0.3028326023391813
r=:0.3245926848366494
f=:0.30382021360092293
Confusion Matrix:
predict 0	1	2	3
true 0	[340, 	57, 	0, 	0]
     1	[148, 	118, 	1, 	0]
     2	[47, 	10, 	0, 	0]
     3	[41, 	5, 	0, 	0]

#############################################################################################
Analysis					
#############################################################################################

We made a feature-classifier based tweet many detection prototype for the first stage. 
In the feature selection part. We have 18 features now (polarity, subjectivity, similarity, 
discourse marker, intensifier, politcal, celebrity, adjective/adverb, preposition, 
punctuation, word count, laughter, named entity, stopwords, swear words, URLs, interjections).
These features are measured separately and concatenated as one vector to be the input of 
classifiers. The classifiers takes the feature vector as input and output a binary categorical 
label for each tweet in subtask A and output a multiclass categorical label for each tweet in 
subtask B. Now the system has 4 embedded classifiers (Logistic regression, Support 
vector machine, Random forest and voting system consisting of the first three). It is surpring 
that SVM is initially very good for a short feature list but becomes worse when the feature list 
grows long, and then logistic regression becomes the best one. Our final result is that Logistic 
Regression is the best for subtask A and voting system is the best for subtask B. 
For subtask A, the accuracies for 4 classifiers are 0.650, 0.596, 0.622, 0.644, respectively. 
For subtask B, the accuracies for 4 classifiers are 0.603, 0.545, 0.584, 0.607, respectively.
Based on the results, we will use logistic regression only for subtask A and use a voting system
for subtask B.

In order to have a taste of the reasons behind the different performance of the classifiers. 
We make confusion matrices for all the classifiers and do a weight analysis for logistic 
regression model because logistic regression has a linear kernel. From the confusion matrix, 
random forest and logistic regression classifiers tend to overlook more irony tweets rather 
than marking tweets as irony. The SVM classifier tends to detect more irony. Hence we combine
them in a voting system to enhance each other. But it does not always work out well. We believe
that this is because SVM and Random Forest are both on the weak side, so they neutralize Logistic
Regression's advantages. 

We try to understand the importance of the features by the magnitude of their weights in 
logistic regression. Both of them in subtask A and B are shown in the above results, respectively. 
This means of the features we have now, the stop word and laughter are a relative weak features. 
Intensifier, discourse marker, adjective/adverb, preposition are the strong features. 

While the accuracy for subtask B does not seem bad, but when we see the confusion matrix, we realize
that our system does not achive to much in subtask B as required by the competition. 
In subtask B, we need to label a tweet with 0(non-ironic) or 1,2,3 (three different subcategories 
of irony). However, the difference among these subcategories are so subtle that our features does 
not capture them very well. And overall these classifiers have a hard time on multiclass classification. 
We review our system and the output of our system and find several possible reasons:
1. From the confusion matrix, we can see that the class 2(7%) and 3(5.9%) are relatively rare. This makes the 
task very hard for classifiers because of lack of information for class 2 and 3. 
2. We choose tweet from class 1, 2 and 3(different ironies) and find that their feature lists are more 
similar to each other than to class 0(non-ironic). This means we miss features that are revealing for
identifying different kinds of irony.
But because the majority class of the tweets are class 0 and class 1. We are still able to get a high accuracy(0.6+).

Another phenomenon in subtask B is that the precision, recall, f-measure scores are very low, 
compared with accuracy. This is because according to the definition of p, r, f, they are most 
meaningful and should be close to accuracy only in binary classification. We assume this is the 
reason why the official evaluation function does not provide a translation of these measures in 
subtask B. We simply use the official evaluation function with default parameter setting and get 
these values. We compare our result to reported results on Codalab and find the same issue are also
seen in other teams' results.

We also try to include a dimension reduction step before passing the feature list to classifiers, 
however that does not improve the performance too much.

There are still many parameters such as bag of words dimension and pca dimension we can fine-tune 
in our system to optimize the performance. But the improvements of these fine-tunings are not
significant. 

#############################################################################################
#############################################################################################
